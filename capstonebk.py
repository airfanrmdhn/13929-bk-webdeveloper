# -*- coding: utf-8 -*-
"""capstoneBK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RHbJNV12KfiW95SCd3kGRVWFlGERU-ey

Nama : Ahnaf Irfan Ramadhan
Nim : A11.2021.13929

1. pengumpulan data
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.utils import resample

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/water_potability.csv')

df

"""2. menelaah data"""

#tampilan informasi data
print('Dataset Information:')
df.info()

#tampilan deskripsi data
df.describe()

#tampilan uniqe
df.nunique()

#tampilan tipe data
df.dtypes

"""3. visualisasi & validasi data"""

#cek missing value
missing_values = df.isnull().sum()
print("Missing Values :\n", missing_values)

df.fillna(df.mean(), inplace=True)

#isi numerik dengan mean
df.fillna(df.mean(), inplace=True)
df.isnull().sum()

def grab_col_names(dataframe, cat_th = 10, car_th = 20):
  #pemisahkan kolom dalam dataset

    cat_cols = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["category", "object", "bool"]]
    # kolom kategorikal (tipe "category", "object", "bool", atau numerik dengan unique values < cat_th)
    num_but_cat = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["int64", "float64"] and dataframe[col].nunique() < cat_th]
    #kolom numerik yang bertindak seperti kategorikal (berdasarkan jumlah unique values)
    cat_but_car = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["category", "object"] and dataframe[col].nunique() > car_th]
    #kolom kategorikal dengan unique values > car_th (high cardinality)

    cat_cols = cat_cols + num_but_cat
    cat_cols = [col for col in cat_cols if col not in cat_but_car]

    # kolom numerik murni (int64, float64) yang bukan kategorikal
    num_cols = [col for col in dataframe.columns if str(dataframe[col].dtypes) in ["int64", "float64"]]
    num_cols = [col for col in num_cols if col not in cat_cols]

    print(f"Jumlah observasi : {dataframe.shape[0]}")
    print(f"Jumlah variabel : {dataframe.shape[1]}")
    print(f"Kolom kategorikal : {len(cat_cols)}")
    print(f"Kolom Numerik : {len(num_cols)}")
    print(f"Kategori tapi kardinal : {len(cat_but_car)}")
    print(f"Numerik tapi kategorikal : {len(num_but_cat)}")

    #pengembalian daftar kolom yang dikelompokkan berdasarkan tipe
    return cat_cols, num_cols, cat_but_car

cat_cols, num_cols, cat_but_car = grab_col_names(df)

#buat visualisasi boxplot untuk semua kolom numerik
def boxplot(dataframe, numeric_columns):

    num_plots = len(numeric_columns)
    num_rows = (num_plots + 3) // 4
    #menghitung jumlah baris
    fig, axes = plt.subplots(num_rows, 4, figsize=(18, 4*num_rows))
    #membuat grid subplots

    for i, column in enumerate(numeric_columns):
        row = i // 4
        col = i % 4
        sns.boxplot(y=dataframe[column], ax=axes[row, col])
        #iterasi Kolom numerik untuk membuat boxplot
        axes[row, col].set_title(f'boxplot dari {column}')
        axes[row, col].set_ylabel(column)

    #penentuan posisi subplot pada grid
    for j in range(num_plots, num_rows*4):
        row = j // 4
        col = j % 4
        fig.delaxes(axes[row, col])
        #menghapus Grafik Kosong

    #penyesuaian Layout
    plt.tight_layout()
    plt.show()

boxplot(df, num_cols)

def detect_outlier_iqr_all_columns(df):

    outliers_dict = {}

    #looping untuk setiap kolom numerik
    for col in df.select_dtypes(include=[np.number]).columns:
        data = df[col].dropna()
        #mengambil data dari kolom dan menghapus nilai yang hilang
        data_sorted = sorted(data)
        #mengurutkan data untuk menghitung kuantil

        #menghitung Q1, Q3, dan IQR
        q1 = np.percentile(data_sorted, 25)
        q3 = np.percentile(data_sorted, 75)
        IQR = q3 - q1

        #menentukan batas bawah dan atas untuk outlier
        lwr_bound = q1 - (1.5 * IQR)
        upr_bound = q3 + (1.5 * IQR)

        #mencari outlier dan menyimpannya ke list
        outliers = [i for i in data_sorted if i < lwr_bound or i > upr_bound]

        #apabila ada outlier, tambahkan ke kamus outliers_dict
        if outliers:
            outliers_dict[col] = outliers

    return outliers_dict

outliers_found = detect_outlier_iqr_all_columns(df)

for col, outliers in outliers_found.items():
    print(f"Outliers dalam {col}  (total: {len(outliers)})")

from imblearn.over_sampling import SMOTE

X = df.drop('Potability', axis=1)
#fitur
y = df['Potability']
#target

smote = SMOTE(random_state=42)
#inisialisasi smote dengan random state
X_resampled, y_resampled = smote.fit_resample(X, y)
#resampling data

resampled_data = pd.DataFrame(X_resampled, columns=X.columns)
resampled_data['Potability'] = y_resampled

#sebelum resampling
before_resampling = df['Potability'].value_counts()

#setelah resampling
after_resampling = resampled_data['Potability'].value_counts()


comparison_df = pd.DataFrame({
    'Sebelum Resampling': before_resampling,
    'Setelah Resampling': after_resampling
})
comparison_df.index = ['tidak layak konsumsi (0)', 'layak konsumsi (1)']


comparison_df.plot(kind='bar', figsize=(10, 6))
plt.title('perbandingan data sebelum dan setelah resampling')
plt.xlabel('kualitas air')
plt.ylabel('total sampel')
plt.xticks(rotation=0)
plt.legend(loc='upper right')
plt.show()

"""4. menentukan objek"""

from sklearn.model_selection import train_test_split


#fitur (X)
X = df[['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']]

#target (y)
y = df['Potability']


#pembagian data menjadi data training dan testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""5. membersihkan data"""

#penghitungan matriks korelasi
correlation = df.corr()

#pembuatan heatmap
plt.figure(figsize=(15, 12))
sns.heatmap(correlation, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('correlation heatmap')
plt.show()

#fitur yang akan divisualisasi
features = ['ph', 'Hardness', 'Solids', 'Chloramines', 'Sulfate', 'Conductivity', 'Organic_carbon', 'Trihalomethanes', 'Turbidity']

fig, axes = plt.subplots(3, 3, figsize=(15, 15))  # 3 baris, 3 kolom

#flatten axes untuk iterasi
axes = axes.flatten()

#loop melalui setiap fitur dan buat histogram di subplot
for i, feature in enumerate(features):
    sns.histplot(data=df, x=feature, kde=True, ax=axes[i])
    axes[i].set_title(f'Distribusi {feature}')
    axes[i].set_xlabel(feature)
    axes[i].set_ylabel('Frekuensi')


plt.tight_layout()
plt.show()

"""7. pemodelan"""

from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier


clean_classifier_nb = GaussianNB()
clean_classifier_nb .fit(X_train, y_train)

clean_classifier_dt = DecisionTreeClassifier(random_state=42)
clean_classifier_dt .fit(X_train, y_train)

clean_classifier_rf = RandomForestClassifier(n_estimators=100, random_state=42)
clean_classifier_rf .fit(X_train, y_train)

from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score,recall_score,f1_score,precision_score,roc_auc_score,confusion_matrix,precision_score

def evaluation(Y_test,Y_pred):
    acc = accuracy_score(Y_test,Y_pred)
    rcl = recall_score(Y_test,Y_pred,average = 'weighted')
    f1 = f1_score(Y_test,Y_pred,average = 'weighted')
    ps = precision_score(Y_test,Y_pred,average = 'weighted')

    metric_dict={'accuracy': round(acc,3),
               'recall': round(rcl,3),
               'F1 score': round(f1,3),
               'Precision score': round(ps,3)
              }

    return print(metric_dict)

y_pred_nb = clean_classifier_nb.predict(X_test)

#evaluate naive bayes model
print("\nNaive Bayes Model:")
accuracy_nb = round(accuracy_score(y_test, y_pred_nb),3)
print("Accuracy:",accuracy_nb)
print("Classification Report:")
print(classification_report(y_test, y_pred_nb))

evaluation(y_test,y_pred_nb)

cm_nb = confusion_matrix(y_test, y_pred_nb)

#membuat visualisasi
plt.figure(figsize=(8, 6))
sns.heatmap(cm_nb, annot=True, fmt="d", cmap="Blues", xticklabels=["Not Potable", "Potable"], yticklabels=["Not Potable", "Potable"])
plt.title("confusion matrix - naive bayes")
plt.xlabel("Prediksi")
plt.ylabel("Aktual")
plt.show()

y_pred_dt = clean_classifier_dt.predict(X_test)

#evaluate random forest model
print("\nRandom Forest Model:")
accuracy_dt = round(accuracy_score(y_test, y_pred_dt),3)
print("Accuracy:",accuracy_dt)
print("Classification Report:")
print(classification_report(y_test, y_pred_dt))

evaluation(y_test,y_pred_dt)

cm_dt = confusion_matrix(y_test, y_pred_dt)

#membuat visualisasi
plt.figure(figsize=(8, 6))
sns.heatmap(cm_dt, annot=True, fmt="d", cmap="Blues", xticklabels=["Not Potable", "Potable"], yticklabels=["Not Potable", "Potable"])
plt.title("confusion matrix - decision tree")
plt.xlabel("Prediksi")
plt.ylabel("Aktual")
plt.show()

y_pred_rf = clean_classifier_rf.predict(X_test)

#evaluate Random Forest model
print("\nRandom Forest Model:")
accuracy_rf = round(accuracy_score(y_test, y_pred_rf),3)
print("Accuracy:",accuracy_rf)
print("Classification Report:")
print(classification_report(y_test, y_pred_rf))

evaluation(y_test,y_pred_rf)

cm_rf = confusion_matrix(y_test, y_pred_rf)

#membuat visualisasi
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt="d", cmap="Blues", xticklabels=["Not Potable", "Potable"], yticklabels=["Not Potable", "Potable"])
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Prediksi")
plt.ylabel("Aktual")
plt.show()

"""8. evaluasi"""

model_comp = pd.DataFrame({'Model': ['Naive Bayes','Decision Tree','Random Forest'], 'Accuracy': [accuracy_nb*100,
                    accuracy_dt*100,accuracy_rf*100]})

#membuat bar plot berketerangan jumlah
fig, ax = plt.subplots()
bars = plt.bar(model_comp['Model'], model_comp['Accuracy'], color=['red', 'green', 'blue'])
plt.xlabel('Model')
plt.ylabel('Accuracy (%)')
plt.title('Sebelum di Normalisasi')
plt.xticks(rotation=45, ha='right')



for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')

plt.show()

from sklearn.preprocessing import StandardScaler

#inisialisasi StandardScaler
scaler = StandardScaler()

#melakukan fit dan transform pada data training
X_train_scaled = scaler.fit_transform(X_train)

#melakukan transform pada data testing (menggunakan parameter yang dipelajari dari data training)
X_test_scaled = scaler.transform(X_test)

#sebelum dilakukan normalisasi

#akurasi model sebelum normalisasi
accuracy_nb = round(accuracy_score(y_test, y_pred_nb), 3)
accuracy_dt = round(accuracy_score(y_test, y_pred_dt), 3)
accuracy_rf = round(accuracy_score(y_test, y_pred_rf), 3)

#setelah normalisasi

#prediksi model setelah normalisasi
y_pred_nbN = clean_classifier_nb.predict(X_test_scaled)
y_pred_dtN = clean_classifier_dt.predict(X_test_scaled)
y_pred_rfN = clean_classifier_rf.predict(X_test_scaled)

#akurasi model setelah normalisasi
accuracy_nbN = round(accuracy_score(y_test, y_pred_nbN), 3)
accuracy_dtN = round(accuracy_score(y_test, y_pred_dtN), 3)
accuracy_rfN = round(accuracy_score(y_test, y_pred_rfN), 3)

#perbandingan akurasi
#membuat DataFrame untuk perbandingan
data = {
    'Model': ['Naive Bayes', 'Decision Tree', 'Random Forest'],
    'Akurasi Sebelum Normalisasi': [accuracy_nb, accuracy_dt, accuracy_rf],
    'Akurasi Setelah Normalisasi': [accuracy_nbN, accuracy_dtN, accuracy_rfN]
}

comparison_df = pd.DataFrame(data)
print(comparison_df)

models = ['Naive Bayes', 'Decision Tree', 'Random Forest']
before_normalization = [accuracy_nb, accuracy_dt, accuracy_rf]
after_normalization = [accuracy_nbN, accuracy_dtN, accuracy_rfN]

#menentukan posisi bar
x = np.arange(len(models))
width = 0.35

#membuat bar plot
fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, before_normalization, width, label='Sebelum Normalisasi')
rects2 = ax.bar(x + width/2, after_normalization, width, label='Setelah Normalisasi')

#menambahkan label, judul, dan legend
ax.set_ylabel('Akurasi')
ax.set_title('Perbandingan Akurasi Model Sebelum dan Sesudah Normalisasi')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()

#menambahkan label nilai di atas setiap bar
def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)
fig.tight_layout()
plt.show()

"""10. kesimpulan

#*Akurasi*
Algoritma	\\ Akurasi (Sebelum Normalisasi)	\\ Akurasi (Setelah Normalisasi)

**Naive Bayes** \\ 63.1% \\ 37.2%

**Decision Tree** \\ 57.8% \\ 62.8%

**Random Forest** \\ 67.8% \\ 37.2%

#*Keunggulan dan Keterbatasan*

**Naive Bayes:**

Keunggulan: Sederhana, cepat, dan cocok untuk dataset dengan dimensi tinggi.
Keterbatasan: Membutuhkan asumsi independensi antar fitur dan sensitif terhadap fitur yang tidak relevan.

**Decision Tree:**

Keunggulan: Mudah dipahami, mampu menangani data kategorikal maupun numerik.
Keterbatasan: Cenderung overfitting dan kurang stabil.

**Random Forest:**

Keunggulan: Mampu mengurangi overfitting, stabil, serta dapat menangani data hilang dan outlier.


Keterbatasan: Lebih kompleks dan membutuhkan waktu pelatihan yang lama.
Rekomendasi Algoritma

#*Rekomendasi*
**Random Forest disarankan untuk kasus ini karena:**

Akurasi Tertinggi: Mencapai akurasi hampir 68%, lebih baik dibandingkan Naive Bayes dan Decision Tree.

Robust dan Stabil: Lebih andal untuk memprediksi data baru tanpa mudah dipengaruhi oleh outlier.

Generalisasi yang Baik: Dapat digunakan pada dataset lain dengan karakteristik serupa.
"""